# Todo split into different sections
batch_size: 12
block_size: 1024
vocab_size: 50304
n_layers: 6
n_head: 6
n_embed: 384
dropout: 0.2
bias: True
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
learning_rate: 0.001
device: "cpu"
dataset: "tiny_shakespeare"
compile: False
decay_lr: True
warmup_iters: 100
lr_decay_iters: 5000
min_lr: 0.0001
max_iters: 5000
eval_interval: 250
eval_iters: 200
log_interval: 10
gradient_accumulation_steps: 5
grad_clip: 1.0
