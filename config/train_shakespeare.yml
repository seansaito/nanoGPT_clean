# Todo split into different sections
batch_size: 64
block_size: 256
vocab_size: 50304
n_layers: 6
n_head: 6
n_embed: 384
dropout: 0.2
bias: True
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
learning_rate: 1e-3
device: "cuda"
dataset: "shakespeare"
compile: False
decay_lr: True
warmup_iters: 100
lr_decay_iters: 5000
min_lr: 1e-4
max_iters: 5000
eval_interval: 250
eval_iters: 200
log_interval: 10
gradient_accumulation_steps: 1
grad_clip: 1.0
